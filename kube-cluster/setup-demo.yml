- hosts: localhost
  gather_facts: False
  vars:
    - application: k8s-cluster
    - ELBNAME: "{{ application }}-load-balancer"

  tasks:
    - name: Create the ELB only listening over port 80
      ec2_elb_lb:
        name: "{{ ELBNAME }}"
        state: present
        region: us-east-2
        zones:
          - us-east-2a
          - us-east-2b
        listeners:
          - protocol: http
            load_balancer_port: 80
            instance_port: 30007
      register: elb

    - name: Create EC2 instances for master
      ec2:
        key_name: pipeline
        instance_type: t2.large
        image: ami-0fc20dd1da406780b
        user_data: "{{ lookup('file', 'user_data.sh') }}"
        region: us-east-2
        group_id: sg-0cfe69542efa6f501
        wait: yes
        instance_tags:
          Name: master
          application: "{{ application }}"
        exact_count: 1
        count_tag:
          Name: master
      register: ec2_master
    - name: Wait for SSH to come up
      local_action: wait_for host={{ item.public_ip }}
        port=22 delay=60 timeout=320 state=started
      with_items: "{{ ec2_master.instances }}"
    - name: Add all instance public IPs to host
      add_host:
        hostname: master
        ansible_host: "{{ item.public_ip }}"
        groups: ec2master
      loop: "{{ ec2_master.instances }}"

- hosts: localhost
  gather_facts: False
  vars:
    - application: k8s-cluster
    - ELBNAME: "{{ application }}-load-balancer"
  tasks:

    - name: Create EC2 instances for worker nodes
      ec2:
        key_name: pipeline
        instance_type: t2.large
        image: ami-0fc20dd1da406780b
        user_data: "{{ lookup('file', 'user_data.sh') }}"
        region: us-east-2
        group_id: sg-0cfe69542efa6f501
        wait: yes
        instance_tags:
          Name: worker
          application: "{{ application }}"
        exact_count: 2
        count_tag:
          Name: worker
      register: ec2_worker
    - name: Wait for SSH to come up
      local_action: wait_for host={{ item.public_ip }}
        port=22 delay=60 timeout=320 state=started
      with_items: "{{ ec2_worker.instances }}"
    - name: Add all instance public IPs to host
      add_host: hostname={{ item.public_ip }} groups=ec2workers
      loop: "{{ ec2_worker.instances }}"

- hosts: ec2workers:ec2master
  name: Install dependancies
  user: ubuntu
  become: yes
  gather_facts: true
  tasks:
    - name: install Docker
      apt:
        name: docker.io
        state: present
        update_cache: true
    - name: install APT Transport HTTPS
      apt:
        name: apt-transport-https
        state: present

    - name: add Kubernetes apt-key
      apt_key:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present

    - name: add Kubernetes' APT repository
      apt_repository:
        repo: deb http://apt.kubernetes.io/ kubernetes-xenial main
        state: present
        filename: 'kubernetes'

    - name: install kubelet
      apt:
        name: kubelet=1.14.0-00
        state: present
        update_cache: true

    - name: install kubeadm
      apt:
        name: kubeadm=1.14.0-00
        state: present

- hosts: ec2master
  name: Install dependancies
  user: ubuntu
  become: yes
  gather_facts: true
  tasks:
    - name: install kubectl
      apt:
        name: kubectl=1.14.0-00
        state: present
        force: yes

- hosts: ec2master
  become: yes
  tasks:
    - name: initialize the cluster
      shell: kubeadm init --pod-network-cidr=192.168.0.0/16 >> cluster_initialized.txt
      args:
        chdir: $HOME
        creates: cluster_initialized.txt

    - name: create .kube directory
      become: yes
      become_user: ubuntu
      file:
        path: $HOME/.kube
        state: directory
        mode: 0755

    - name: copy admin.conf to user's kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/ubuntu/.kube/config
        remote_src: yes
        owner: ubuntu

    - name: install Pod network
      become: yes
      become_user: ubuntu
      shell: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml >> pod_network_setup.txt
      args:
        chdir: $HOME
        creates: pod_network_setup.txt

- hosts: ec2master
  become: yes
  gather_facts: true
  tasks:
    - name: get join command
      shell: kubeadm token create --print-join-command
      register: join_command_raw

    - name: set join command
      set_fact:
        join_command: "{{ join_command_raw.stdout_lines[0] }}"


- hosts: ec2workers
  become: yes
  tasks:
    - name: TCP port 6443 on master is reachable from worker
      wait_for: "host={{ hostvars['master']['ansible_default_ipv4']['address'] }} port=6443 timeout=1"
    - name: join cluster
      shell: "{{ hostvars['master'].join_command }} >> node_joined.txt"
      args:
        chdir: $HOME
        creates: node_joined.txt

- hosts: ec2master
  name: Deploy app
  user: ubuntu
  gather_facts: false
  tasks:
    - name: clone git repo
      git:
        repo: 'https://github.com/donkodimov/ml-microservice-kubernetes.git'
        dest: /home/ubuntu/repo
        update: yes

    - name: Wait until the deployment file is present before continuing
      wait_for:
        path: /home/ubuntu/repo/prediction-deployment.yaml
        state: present

    - name: Run deployment
      shell: kubectl apply -f /home/ubuntu/repo/prediction-deployment.yaml > deploy_out.txt
      args:
        chdir: $HOME
        creates: deploy_out.txt

    - name: Run service
      shell: kubectl apply -f /home/ubuntu/repo/prediction-service.yaml > svc_out.txt
      args:
        chdir: $HOME
        creates: svc_out.txt

- hosts: ec2workers
  gather_facts: True
  tasks:
    - name: TCP port 30007 on worker nodes is reachable
      wait_for:
        "host={{ ansible_facts['nodename'] }} port=30007"
      delay: 15

- hosts: localhost
  gather_facts: False
  vars:
    - application: k8s-cluster
    - ELBNAME: "{{ application }}-load-balancer"
  tasks:
    - name: Add each EC2 instance to the ELB
      ec2_elb:
        state: present
        ec2_elbs: "{{ ELBNAME }}"
        region: "{{ item.region }}"
        instance_id: "{{ item.id }}"
      with_items: "{{ ec2_worker.tagged_instances }}"